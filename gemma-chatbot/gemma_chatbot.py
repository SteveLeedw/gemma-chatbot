"""
Gemma ì˜¤í”ˆì†ŒìŠ¤ LLMì„ ì‚¬ìš©í•œ ë¡œì»¬ ì±—ë´‡
- ì™„ì „ ë¬´ë£Œ
- ë„¤íŠ¸ì›Œí¬ ë¶ˆí•„ìš” (ì„¤ì¹˜ í›„)
- ê°œì¸ì •ë³´ ë³´í˜¸
- ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ ê¸°ëŠ¥
- íŒŒì¼ ë²ˆì—­ ê¸°ëŠ¥
"""

import ollama
import json
import os

# ============================================================
# ëª¨ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ============================================================
SYSTEM_PROMPTS = {
    "ëŒ€í™”": "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.",
    "ë²ˆì—­": """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì˜ì–´â†’í•œêµ­ì–´ ë²ˆì—­ê°€ì…ë‹ˆë‹¤.
ê·œì¹™:
1. ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
2. ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•©ë‹ˆë‹¤.
3. ì „ë¬¸ìš©ì–´ëŠ” í•œêµ­ì–´ í‘œì¤€ ë²ˆì—­ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
4. ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. ì„¤ëª… ë¶ˆí•„ìš”.""",
    "ë¬¸ì„œë²ˆì—­": """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¬¸ì„œ ë²ˆì—­ê°€ì…ë‹ˆë‹¤.
ê·œì¹™:
1. ì˜ì–´ ë¬¸ì„œ ì „ì²´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
2. ì›ë³¸ì˜ í˜•ì‹(ì œëª©, ë‹¨ë½, êµ¬ì¡°)ì„ ìœ ì§€í•©ë‹ˆë‹¤.
3. ì „ë¬¸ìš©ì–´ëŠ” í•œêµ­ì–´ í‘œì¤€ ë²ˆì—­ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
4. ë²ˆì—­ëœ ë¬¸ì„œë§Œ ì¶œë ¥í•©ë‹ˆë‹¤."""
}

def chat_with_gemma(user_message, conversation_history=[], model="gemma2:2b"):
    """
    Gemmaì™€ ëŒ€í™”í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        user_message: ì‚¬ìš©ì ë©”ì‹œì§€
        conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­
        model: ì‚¬ìš©í•  Gemma ëª¨ë¸
    
    Returns:
        AIì˜ ì‘ë‹µê³¼ ì—…ë°ì´íŠ¸ëœ ëŒ€í™” ë‚´ì—­
    """
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” ë‚´ì—­ì— ì¶”ê°€
    conversation_history.append({
        "role": "user",
        "content": user_message
    })
    
    try:
        # Ollamaë¡œ Gemma ëª¨ë¸ í˜¸ì¶œ
        response = ollama.chat(
            model=model,
            messages=conversation_history,
            stream=False  # Trueë¡œ í•˜ë©´ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
        )
        
        # AI ì‘ë‹µ ì¶”ì¶œ
        ai_message = response['message']['content']
        
        # AI ì‘ë‹µì„ ëŒ€í™” ë‚´ì—­ì— ì¶”ê°€
        conversation_history.append({
            "role": "assistant",
            "content": ai_message
        })
        
        return ai_message, conversation_history
        
    except Exception as e:
        error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        if "model" in str(e).lower():
            error_msg += "\n\nğŸ’¡ Gemma ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            error_msg += "\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: ollama pull gemma2:2b"
        return error_msg, conversation_history


def list_available_models():
    """ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ í™•ì¸"""
    try:
        models = ollama.list()
        return models
    except:
        return None


def switch_mode(mode, conversation_history):
    """ëª¨ë“œ ë³€ê²½ ë° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸"""
    if mode in SYSTEM_PROMPTS:
        conversation_history.clear()
        conversation_history.append({
            "role": "system",
            "content": SYSTEM_PROMPTS[mode]
        })
        print(f"\nâœ… [{mode}] ëª¨ë“œë¡œ ì „í™˜ë¨!")
        return True
    else:
        print(f"\nâŒ ì˜ëª»ëœ ëª¨ë“œì…ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {', '.join(SYSTEM_PROMPTS.keys())}")
        return False


def translate_file(filepath, conversation_history, selected_model):
    """íŒŒì¼ì„ ì½ì–´ì„œ ë²ˆì—­ í›„ ì €ì¥"""
    try:
        # íŒŒì¼ ì½ê¸°
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()

        if not content.strip():
            print("\nâŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return

        # ë¬¸ì„œë²ˆì—­ ëª¨ë“œë¡œ ì„ì‹œ ì „í™˜
        temp_history = [{
            "role": "system",
            "content": SYSTEM_PROMPTS["ë¬¸ì„œë²ˆì—­"]
        }]

        # ë²ˆì—­ ì‹¤í–‰
        print("\nğŸ“„ ë²ˆì—­ ì¤‘... (íŒŒì¼ì´ í´ìˆ˜ë¡ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        translated, _ = chat_with_gemma(content, temp_history, model=selected_model)

        # ë²ˆì—­ëœ íŒŒì¼ ì €ì¥
        base, ext = os.path.splitext(filepath)
        output_path = f"{base}_korean{ext}"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(translated)

        print(f"\nâœ… ë²ˆì—­ ì™„ë£Œ!")
        print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {output_path}")
        print(f"\n--- ë²ˆì—­ ê²°ê³¼ ---\n{translated}\n--- ë ---")

    except FileNotFoundError:
        print(f"\nâŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def main():
    """ë©”ì¸ ì±—ë´‡ ë£¨í”„"""
    print("=" * 60)
    print("ğŸ¤– Gemma ë¡œì»¬ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    print("   (ì™„ì „ ë¬´ë£Œ & ì˜¤í”„ë¼ì¸ ì‘ë™)")
    print("=" * 60)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ Gemma ëª¨ë¸ë“¤
    available_models = {
        "1": ("gemma2:2b", "ê°€ì¥ ê°€ë²¼ì›€ (2GB) - ë¹ ë¥¸ ì†ë„"),
        "2": ("gemma2:9b", "ì¤‘ê°„ í¬ê¸° (5GB) - ê· í˜•ì¡íŒ ì„±ëŠ¥"),
        "3": ("gemma2:27b", "í° í¬ê¸° (15GB) - ìµœê³  ì„±ëŠ¥"),
    }
    
    print("\nğŸ“¦ ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:")
    for key, (model, desc) in available_models.items():
        print(f"  {key}. {model} - {desc}")
    
    choice = input("\nì„ íƒ (1/2/3) [ê¸°ë³¸ê°’: 1]: ").strip() or "1"
    selected_model = available_models.get(choice, available_models["1"])[0]
    
    print(f"\nâœ… {selected_model} ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("\nğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
    print("  [ëª¨ë“œ ë³€ê²½]")
    print("    mode ëŒ€í™”      â†’ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
    print("    mode ë²ˆì—­      â†’ ì˜ì–´â†’í•œêµ­ì–´ ë²ˆì—­ ëª¨ë“œ")
    print("    mode ë¬¸ì„œë²ˆì—­   â†’ ë¬¸ì„œ ì „ì²´ ë²ˆì—­ ëª¨ë“œ")
    print("  [íŒŒì¼ ë²ˆì—­]")
    print("    file íŒŒì¼ê²½ë¡œ   â†’ ì˜ì–´ íŒŒì¼ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­")
    print("    ì˜ˆ: file C:\\Users\\Downloads\\test.txt")
    print("  [ê¸°íƒ€]")
    print("    save            â†’ ëŒ€í™” ë‚´ì—­ ì €ì¥")
    print("    reset           â†’ ëŒ€í™” ì´ˆê¸°í™”")
    print("    ì¢…ë£Œ / exit     â†’ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("=" * 60)
    
    # ê¸°ë³¸ ëª¨ë“œ: ë²ˆì—­
    current_mode = "ë²ˆì—­"
    conversation_history = [{
        "role": "system",
        "content": SYSTEM_PROMPTS[current_mode]
    }]
    print(f"\nâœ… ê¸°ë³¸ ëª¨ë“œ: [{current_mode}]")
    print("ğŸ’¡ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ë„£ê¸°í•˜ë©´ ë°”ë¡œ ë²ˆì—­ë©ë‹ˆë‹¤!")
    
    while True:
        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        user_input = input("\në‹¹ì‹ : ").strip()
        
        # ì¢…ë£Œ ëª…ë ¹ í™•ì¸
        if user_input.lower() in ['ì¢…ë£Œ', 'exit', 'quit']:
            print("\nì±—ë´‡: ì•ˆë…•íˆ ê°€ì„¸ìš”! ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”! ğŸ‘‹")
            break
        
        # ëŒ€í™” ì €ì¥
        if user_input.lower() == 'save':
            filename = f"gemma_conversation_{len(conversation_history)}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(conversation_history, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ëŒ€í™”ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            continue
        
        # ëª¨ë“œ ë³€ê²½
        if user_input.lower().startswith("mode "):
            mode = user_input[5:].strip()
            if switch_mode(mode, conversation_history):
                current_mode = mode
            continue

        # íŒŒì¼ ë²ˆì—­
        if user_input.lower().startswith("file "):
            filepath = user_input[5:].strip().strip('"').strip("'")
            translate_file(filepath, conversation_history, selected_model)
            continue
        
        # ëŒ€í™” ì´ˆê¸°í™” (í˜„ì¬ ëª¨ë“œ ìœ ì§€)
        if user_input.lower() == 'reset':
            conversation_history.clear()
            conversation_history.append({
                "role": "system",
                "content": SYSTEM_PROMPTS[current_mode]
            })
            print(f"\nğŸ”„ ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤! (í˜„ì¬ ëª¨ë“œ: {current_mode})")
            continue
        
        if not user_input:
            continue
        
        # AI ì‘ë‹µ ë°›ê¸°
        print("\nì±—ë´‡: ", end="", flush=True)
        ai_response, conversation_history = chat_with_gemma(
            user_input, 
            conversation_history,
            model=selected_model
        )
        
        print(ai_response)


if __name__ == "__main__":
    main()
